{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./src/\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ot as pot\n",
    "import torch\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "\n",
    "from DANN import DANN\n",
    "from DAN import DAN\n",
    "from CORAL import coral\n",
    "from optimalTransport1D import optimalTransport1D\n",
    "from nnModel import FC_embedding, EmbeddingModel\n",
    "\n",
    "from utils import *\n",
    "from io_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DanseNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.hidden_rep = self.input_layer(inputs)\n",
    "        return self.output_layer(self.hidden_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pd.read_csv(\"./data/train_transaction.csv\")\n",
    "identity = pd.read_csv(\"./data/train_identity.csv\")\n",
    "trans = trans.merge(identity[[\"TransactionID\", \"DeviceType\"]], on=\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = trans[trans.DeviceType==\"mobile\"]\n",
    "target = trans[trans.DeviceType==\"desktop\"]\n",
    "\n",
    "nan_percent = trans.isnull().sum(axis=0) / trans.shape[0]\n",
    "\n",
    "ignore = nan_percent[nan_percent > 0.01].index.values.tolist() + [\"isFraud\", \"TransactionDT\", \"DeviceType\"]\n",
    "\n",
    "source_label = source[\"isFraud\"].values\n",
    "target_label = target[\"isFraud\"].values\n",
    "\n",
    "source = source[[f for f in source.columns if f not in ignore]]\n",
    "target = target[[f for f in target.columns if f not in ignore]]\n",
    "\n",
    "source = pd.merge(identity, source, how=\"right\", on=\"TransactionID\")\n",
    "target = pd.merge(identity, target, how=\"right\", on=\"TransactionID\")\n",
    "\n",
    "nan_percent = source.append(target).isnull().sum(axis=0) / source.append(target).shape[0]\n",
    "\n",
    "ignore = nan_percent[nan_percent > 0.01].index.values.tolist() + [\"TransactionID\", \"DeviceType\"]\n",
    "\n",
    "source = source[[f for f in source.columns if f not in ignore]]\n",
    "target = target[[f for f in target.columns if f not in ignore]]\n",
    "\n",
    "source_index = np.where(~np.any(source.isnull().values, axis=1))[0]\n",
    "target_index = np.where(~np.any(target.isnull().values, axis=1))[0]\n",
    "\n",
    "cates = [\"id_12\", \"id_15\", \"id_28\", \"id_29\", \"id_31\", \"id_35\", \"id_36\", \"id_37\", \"id_38\", \n",
    "         \"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\"]\n",
    "no_cates = [c for c in source.columns if c not in cates]\n",
    "source = source[cates+no_cates]\n",
    "target = target[cates+no_cates]\n",
    "\n",
    "for c in cates:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(source[c].append(target[c]).astype(str))\n",
    "    source[c] = encoder.transform(source[c].astype(str))\n",
    "    target[c] = encoder.transform(target[c].astype(str))\n",
    "    \n",
    "cates = [\"id_15\", \"id_31\",\"ProductCD\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\"]\n",
    "no_cates = [c for c in source.columns if c not in cates]\n",
    "source = source[cates+no_cates]\n",
    "source.drop(\"card1\", inplace=True, axis=1)\n",
    "target = target[cates+no_cates]\n",
    "target.drop(\"card1\", inplace=True, axis=1)\n",
    "\n",
    "source = source.values[source_index]\n",
    "target = target.values[target_index]\n",
    "\n",
    "source_label = source_label[source_index]\n",
    "target_label = target_label[target_index]\n",
    "\n",
    "\n",
    "min_values = np.min(np.r_[source, target], axis=0)\n",
    "\n",
    "source = source - min_values\n",
    "target = target - min_values\n",
    "\n",
    "for i in range(8, 120):\n",
    "    source[:,i] = np.log(1 + source[:,i])\n",
    "    target[:,i] = np.log(1 + target[:,i])\n",
    "    \n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "embd = FC_embedding()\n",
    "\n",
    "params = {\n",
    "    \"epoch\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"pos_weight\": 1,\n",
    "    \"model\": embd\n",
    "}\n",
    "model = EmbeddingModel(**params)\n",
    "\n",
    "source_size = int(source.shape[0] / 4)\n",
    "\n",
    "model.fit(source[:source_size*3], source_label[:source_size*3], \n",
    "          source[source_size*3:], source_label[source_size*3:], verbose=True)\n",
    "\n",
    "model.save_embedding_dict(\"./data/embedding_dict_kaggle.pkl\")\n",
    "\n",
    "np.save(\"./data/mobile_trans\", source)\n",
    "np.save(\"./data/mobile_label\", source_label)\n",
    "np.save(\"./data/desktop_trans\", target)\n",
    "np.save(\"./data/desktop_label\", target_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.load(\"./data/mobile_trans.npy\")\n",
    "target = np.load(\"./data/desktop_trans.npy\")\n",
    "source_label = np.load(\"./data/mobile_label.npy\")\n",
    "target_label = np.load(\"./data/desktop_label.npy\")\n",
    "\n",
    "with open(\"./data/embedding_dict_kaggle.pkl\", \"rb\") as file:\n",
    "    embedding_dict = pickle.load(file)\n",
    "    \n",
    "# exclude target unique modality\n",
    "exclude_index = []\n",
    "for i in range(8):\n",
    "    target_modality = np.unique(target[:, i])\n",
    "    source_modality = np.unique(source[:, i])\n",
    "    exclude = [m for m in target_modality if m not in source_modality]\n",
    "    exclude_index.append(np.isin(target[:, i], exclude))\n",
    "target_cate = target[~np.any(exclude_index, axis=0)]\n",
    "target_num_label = target_label[~np.any(exclude_index, axis=0)]\n",
    "\n",
    "source_cate_embedding = []\n",
    "for i in range(8):\n",
    "    source_cate_embedding.append(embedding_dict[i][\"weight\"].cpu()[source[:, i].astype(int)])\n",
    "    \n",
    "target_cate_embedding = []\n",
    "for i in range(8):\n",
    "    target_cate_embedding.append(embedding_dict[i][\"weight\"].cpu()[target_cate[:, i].astype(int)])\n",
    "    \n",
    "source_num = np.concatenate(source_cate_embedding, axis=1)\n",
    "source_num = np.c_[source_num, source[:, 8:]]\n",
    "\n",
    "target_num = np.concatenate(target_cate_embedding, axis=1)\n",
    "target_num = np.c_[target_num, target_cate[:, 8:]]\n",
    "\n",
    "target_num_size = int(target_num.shape[0] // 12)\n",
    "\n",
    "del(target)\n",
    "del(target_label)\n",
    "gc.collect()\n",
    "target = target_cate\n",
    "target_label = target_num_label\n",
    "target_size = target_num_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_adaptation(target, source):\n",
    "    \"\"\"\n",
    "    input: target & source with categorical features\n",
    "    return: adapted embed target\n",
    "    \"\"\"\n",
    "    ot_plans = []\n",
    "    source_modalities = []\n",
    "    target_modalities = []\n",
    "\n",
    "    for c in range(8):\n",
    "        sim, modality = of_uni_cate(np.r_[target[:,c], source[:,c]])\n",
    "        distance = similarity_to_dissimilarity(sim)\n",
    "\n",
    "        target_modality, counts = np.unique(target[:,c], return_counts=True)\n",
    "        target_density = counts / counts.sum()\n",
    "        target_modalities.append(target_modality)\n",
    "\n",
    "        source_modality, counts = np.unique(source[:,c], return_counts=True)\n",
    "        source_density = counts / counts.sum()\n",
    "        source_modalities.append(source_modality)\n",
    "\n",
    "        Gs = pot.emd(target_density.tolist(), source_density.tolist(), \n",
    "                     distance[np.where(np.in1d(modality, target_modality))[0]][:,np.where(np.in1d(modality, source_modality))[0]].tolist())\n",
    "\n",
    "        norm_array = Gs.sum(axis=1)\n",
    "        norm_array[norm_array==0] = 1\n",
    "        ot_plan = (Gs.T / norm_array).T\n",
    "        ot_plans.append(ot_plan)\n",
    "\n",
    "\n",
    "    target_embedding = []\n",
    "    for c, ot_plan in enumerate(ot_plans):\n",
    "        trans_matrix = np.zeros((embedding_dict[c]['weight'].shape[0], embedding_dict[c]['weight'].shape[0]))\n",
    "        for i, target_index in enumerate(target_modalities[c]):\n",
    "            trans_matrix[int(target_index), list(map(int, source_modalities[c]))] = ot_plan[i]\n",
    "\n",
    "        target_embedding.append(trans_matrix.dot(embedding_dict[c]['weight'].cpu().numpy()))\n",
    "\n",
    "    target_cate_embedding = []\n",
    "    for i in range(8):\n",
    "        target_cate_embedding.append(target_embedding[i][target[:, i].astype(int)])\n",
    "\n",
    "    target_cate_num = np.concatenate(target_cate_embedding, axis=1)\n",
    "    target_cate_num = np.c_[target_cate_num, target[:, 8:]]\n",
    "    return target_cate_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_splits = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=12345)\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(source)):\n",
    "    source_splits.append((train_index, valid_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0 #0, 1, 2, 3\n",
    "train_index, valid_index = source_splits[s]\n",
    "\n",
    "xs, ys = source_num[train_index], source_label[train_index]\n",
    "xv, yv = source_num[valid_index], source_label[valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train baseline NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "try:\n",
    "    clf = load_model(\"./model/kaggle_nn_{}\".format(s))\n",
    "except:\n",
    "    dansnet = DanseNet(input_dim=xs.shape[1], output_dim=1)\n",
    "    clf = DAN(dansnet, device=torch.device(\"cuda\"))\n",
    "    clf.fit(xs, ys, xs, xv, yv, epoch=100, batch_size=128, lr=0.01, beta=0, early_stop=False, verbose=True)\n",
    "    save_model(clf, \"./model/kaggle_nn_{}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No retraining adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5912323528278833"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identity\n",
    "pred_test = clf.predict_prob(target_num)\n",
    "performance(pred_test, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5872259859741512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D OT num\n",
    "opt = optimalTransport1D()\n",
    "tsf, _ = opt.fit_transform(target_num[:,17:], xs[:,17:], njobs=20)\n",
    "x_ot_num = np.c_[target_num[:,:17], tsf]\n",
    "pred_num = clf.predict_prob(x_ot_num)\n",
    "performance(pred_num, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6218406030378054"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D OT cate\n",
    "x_ot_cate = cate_adaptation(target, source[train_index])\n",
    "pred_cate = clf.predict_prob(x_ot_cate)\n",
    "performance(pred_cate, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631372621120201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D OT\n",
    "x_ot = np.c_[x_ot_cate[:,:17], tsf]\n",
    "pred_ot = clf.predict_prob(x_ot)\n",
    "performance(pred_ot, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5489617944336429"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CORAL\n",
    "x_coral = coral(target_num, xs)\n",
    "pred_coral = clf.predict_prob(x_coral)\n",
    "performance(pred_coral, target_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAN\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "try:\n",
    "    clf = load_model(\"./model/kaggle_dan_{}\".format(s))\n",
    "except:\n",
    "    dansnet = DanseNet(input_dim=xs.shape[1], output_dim=1)\n",
    "    clf = DAN(dansnet, device=torch.device(\"cuda\"))\n",
    "    indext = reduce_dataset(target_num, xs)\n",
    "    clf.fit(xs, ys, target_num[indext], xv, yv, epoch=100, batch_size=128, lr=0.01, beta=0.5, early_stop=False, verbose=False)\n",
    "    save_model(clf, \"./model/kaggle_dan_{}\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488761052360881"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DAN\n",
    "pred_dan = clf.predict_prob(target_num)\n",
    "performance(pred_dan, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN \n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "if s == 0:\n",
    "    e, b, a = 50, 1.0, 50\n",
    "elif s == 1:\n",
    "    e, b, a = 50, 1.0, 100\n",
    "elif s == 2:\n",
    "    e, b, a = 100, 0.5, 200\n",
    "else:\n",
    "    e, b, a = 50, 1.0, 200\n",
    "try:\n",
    "    clf = load_model(\"./model/kaggle_dann_{}\".format(s))\n",
    "except:\n",
    "    dansnet = DanseNet(input_dim=xs.shape[1], output_dim=1)\n",
    "    clf = DANN(dansnet, device=torch.device(\"cuda\"))\n",
    "    indext = reduce_dataset(target_num, xs)\n",
    "    clf.fit(xs, ys, target_num[indext], xv, yv, epoch=e, batch_size=128, lr=0.01, beta=b, alpha=a,\n",
    "            early_stop=False, verbose=False)\n",
    "    save_model(clf, \"./model/kaggle_dann_{}\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6398225143360166"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dann = clf.predict_prob(target_num)\n",
    "performance(pred_dann, target_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weakly supervised adaptation with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.01\n",
    "n_sample = 200\n",
    "\n",
    "cate_fea = [(0,), (1, 2, 3), (4,), (5, 6, 7, 8), (9, 10, 11), (12,), (13, 14, 15), (16,)]\n",
    "num_fea = [(i, ) for i in range(17, 129)]\n",
    "\n",
    "for s in range(4):\n",
    "    # Load Dataset\n",
    "    train_index, valid_index = source_splits[s]\n",
    "    xs, ys = source_num[train_index], source_label[train_index]\n",
    "    xv, yv = source_num[valid_index], source_label[valid_index]\n",
    "\n",
    "    # Load Model\n",
    "    clf = load_model(\"kaggle_nn_{}\".format(s))\n",
    "    \n",
    "    # Numerical Adaptation\n",
    "    opt = optimalTransport1D()\n",
    "    tsf, _ = opt.fit_transform(target_num[:,17:], xs[:,17:], njobs=20)\n",
    "\n",
    "    # Categorical Adaptation\n",
    "    x_ot_cate = cate_adaptation(target, source[train_index])\n",
    "    x_ot = np.c_[x_ot_cate[:,:17], tsf]\n",
    "\n",
    "    bootstrap_perf = []\n",
    "    for i in range(30):\n",
    "        # Sample\n",
    "        choice_range = range(target_num.shape[0])\n",
    "        n = int(target_num.shape[0] * ratio)\n",
    "        index = np.random.choice(choice_range, n, replace=False)\n",
    "        \n",
    "        # Compute perf_list and feature_list\n",
    "        perf_list, feature_list, pred_list, perf_dist_list, index_list = greedy_search(clf, target_num[index], x_ot[index], \n",
    "                                                                           target_label[index], \n",
    "                                                                           explore_features=cate_fea + num_fea, \n",
    "                                                                           n_sample=n_sample,\n",
    "                                                                           verbose=False)\n",
    "\n",
    "        save_model(perf_list, \"./model/bootstrap_perf_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(feature_list, \"./model/bootstrap_feature_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(pred_list, \"./model/bootstrap_pred_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(index, \"./model/bootstrap_index_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(perf_dist_list, \"./model/bootstrap_perf_dist_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(index_list, \"./model/bootstrap_index_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        \n",
    "        # Perf on all test data\n",
    "        pred = best_prediction(clf, target_num, x_ot, feature_list, len(feature_list)-1, repeat=1)\n",
    "        perf = performance(pred, target_label)\n",
    "        bootstrap_perf.append(perf)\n",
    "        print(\"Bootstrap:\", perf, len(feature_list))\n",
    "\n",
    "    save_model(bootstrap_perf, \"./model/bootstrap_perf_{}_{}\".format(ratio, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = source[train_index], source_label[train_index]\n",
    "xv, yv = source[valid_index], source_label[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    clf = load_model(\"./model/kaggle_lgb_{}\".format(s))\n",
    "except:\n",
    "    lgb_train = lgb.Dataset(xs, ys, categorical_feature=range(8))\n",
    "    lgb_valid = lgb.Dataset(xv, yv, categorical_feature=range(8))\n",
    "\n",
    "    params = {\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.04,\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 0,\n",
    "        'random_state': 12345\n",
    "    }\n",
    "\n",
    "    clf = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_valid], num_boost_round=5000, \n",
    "                    early_stopping_rounds=50, verbose_eval=10)\n",
    "    save_model(clf, \"./model/kaggle_lgb_{}\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6711815591119094"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identity\n",
    "pred_test = clf.predict(target)\n",
    "performance(pred_test, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6497218833919742"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D OT num\n",
    "opt = optimalTransport1D()\n",
    "tsf, w_distance = opt.fit_transform(target[:,8:], xs[:,8:], njobs=20)\n",
    "x_ot_num = np.c_[target[:,:8], tsf]\n",
    "pred_num = clf.predict(x_ot_num)\n",
    "performance(pred_num, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D OT cate\n",
    "np.random.seed(12345)\n",
    "\n",
    "repeat = 20\n",
    "ot_plans = []\n",
    "transport_plan = []\n",
    "source_modalities = []\n",
    "target_modalities = []\n",
    "\n",
    "for c in range(8):\n",
    "    sim, modality = of_uni_cate(np.r_[target[:,c], xs[:,c]])\n",
    "    distance = similarity_to_dissimilarity(sim)\n",
    "    \n",
    "    target_modality, counts = np.unique(target[:,c], return_counts=True)\n",
    "    target_density = counts / counts.sum()\n",
    "    target_modalities.append(target_modality)\n",
    "\n",
    "    source_modality, counts = np.unique(xs[:,c], return_counts=True)\n",
    "    source_density = counts / counts.sum()\n",
    "    source_modalities.append(source_modality)\n",
    "\n",
    "    Gs = pot.emd(target_density.tolist(), source_density.tolist(), \n",
    "                 distance[np.where(np.in1d(modality, target_modality))[0]][:,np.where(np.in1d(modality, source_modality))[0]].tolist())\n",
    "\n",
    "    norm_array = Gs.sum(axis=1)\n",
    "    norm_array[norm_array==0] = 1\n",
    "    ot_plan = (Gs.T / norm_array).T\n",
    "    ot_plans.append(ot_plan)\n",
    "\n",
    "    mapping = {}\n",
    "    for m in target_modality:\n",
    "        index = np.where(target_modality==m)[0][0]\n",
    "        mapping[m] = source_modality[np.random.choice(range(ot_plan.shape[-1]), size=repeat, p=ot_plan[index])]\n",
    "\n",
    "    transport_plan.append(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for r in range(repeat):\n",
    "    data = target.copy()\n",
    "\n",
    "    for c in range(8):\n",
    "        for m, v in transport_plan[c].items():\n",
    "            data[data[:, c]==m, c] = v[r]\n",
    "\n",
    "    pred = clf.predict(data)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6955292752723653"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cate = np.array(preds).mean(axis=0)\n",
    "performance(pred_cate, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D OT\n",
    "preds = []\n",
    "for r in range(repeat):\n",
    "    data = x_ot_num.copy()\n",
    "\n",
    "    for c in range(8):\n",
    "        for m, v in transport_plan[c].items():\n",
    "            data[data[:, c]==m, c] = v[r]\n",
    "\n",
    "    pred = clf.predict(data)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6704625127135608"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ot = np.array(preds).mean(axis=0)\n",
    "performance(pred_ot, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041227022400151"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CORAL\n",
    "tsf = coral(target[:,8:], xs[:,8:])\n",
    "x_coral = np.c_[target[:,:8], tsf]\n",
    "pred_coral = clf.predict(x_coral)\n",
    "performance(pred_coral, target_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weakly supervised adaptation with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.01\n",
    "n_sample = 200\n",
    "num_fea = [(i, ) for i in range(120)]\n",
    "repeat = 10\n",
    "\n",
    "for s in range(4):\n",
    "    # Load Dataset\n",
    "    train_index, valid_index = source_splits[s]\n",
    "    xs, ys = source[train_index], source_label[train_index]\n",
    "    xv, yv = source[valid_index], source_label[valid_index]\n",
    "\n",
    "    # Load Models\n",
    "    clf = load_model(\"./model/kaggle_lgb_{}\".format(s))\n",
    "    clf.predict_prob = clf.predict\n",
    "\n",
    "    # Numerical transformation\n",
    "    opt = optimalTransport1D()\n",
    "    tsf, w_distance = opt.fit_transform(target[:,8:], xs[:,8:], njobs=20)\n",
    "    x_ot_num = np.c_[target[:,:8], tsf]\n",
    "\n",
    "    # Stochastic categorical transformation\n",
    "    ot_plans = []\n",
    "    transport_plan = []\n",
    "    source_modalities = []\n",
    "    target_modalities = []\n",
    "\n",
    "    for c in range(8):\n",
    "        sim, modality = of_uni_cate(np.r_[target[:,c], xs[:,c]])\n",
    "        distance = similarity_to_dissimilarity(sim)\n",
    "\n",
    "        target_modality, counts = np.unique(target[:,c], return_counts=True)\n",
    "        target_density = counts / counts.sum()\n",
    "        target_modalities.append(target_modality)\n",
    "\n",
    "        source_modality, counts = np.unique(xs[:,c], return_counts=True)\n",
    "        source_density = counts / counts.sum()\n",
    "        source_modalities.append(source_modality)\n",
    "\n",
    "        Gs = pot.emd(target_density.tolist(), source_density.tolist(), \n",
    "                     distance[np.where(np.in1d(modality, target_modality))[0]][:,np.where(np.in1d(modality, source_modality))[0]].tolist())\n",
    "\n",
    "        norm_array = Gs.sum(axis=1)\n",
    "        norm_array[norm_array==0] = 1\n",
    "        ot_plan = (Gs.T / norm_array).T\n",
    "        ot_plans.append(ot_plan)\n",
    "\n",
    "        mapping = {}\n",
    "        for m in target_modality:\n",
    "            index = np.where(target_modality==m)[0][0]\n",
    "            mapping[m] = source_modality[np.random.choice(range(ot_plan.shape[-1]), size=repeat, p=ot_plan[index])]\n",
    "\n",
    "        transport_plan.append(mapping)\n",
    "\n",
    "\n",
    "    bootstrap_perf = []\n",
    "    for i in range(10):\n",
    "        choice_range = range(target.shape[0])\n",
    "        n = int(target.shape[0] * ratio)\n",
    "        index = np.random.choice(choice_range, n, replace=False)\n",
    "\n",
    "        cate_adapt = None\n",
    "        for r in range(repeat):\n",
    "            data = x_ot_num[index].copy()\n",
    "            for c in range(8):\n",
    "                for m, v in transport_plan[c].items():\n",
    "                    data[data[:, c]==m, c] = v[r]\n",
    "            if cate_adapt is None:\n",
    "                cate_adapt = data\n",
    "            else:\n",
    "                cate_adapt = np.r_[cate_adapt, data]\n",
    "\n",
    "        # Compute perf_list and feature_list\n",
    "        perf_list, feature_list, pred_list, perf_dist_list, index_list = greedy_search_cate(clf, \n",
    "                                                                                            target[np.tile(index, repeat)], \n",
    "                                                                                            cate_adapt, \n",
    "                                                                                            target_label[index], \n",
    "                                                                                            explore_features=num_fea, \n",
    "                                                                                            repeat=repeat,\n",
    "                                                                                            n_sample=n_sample,\n",
    "                                                                                            verbose=False)\n",
    "\n",
    "        save_model(perf_list, \"./model/cate_bootstrap_perf_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(feature_list, \"./model/cate_bootstrap_feature_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(pred_list, \"./model/cate_bootstrap_pred_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(index, \"./model/cate_bootstrap_index_{}\".format(ratio, s, i))\n",
    "        save_model(perf_dist_list, \"./model/cate_bootstrap_perf_dist_list_{}_{}_{}\".format(ratio, s, i))\n",
    "        save_model(index_list, \"./model/cate_bootstrap_index_list_{}_{}_{}\".format(ratio, s, i))\n",
    "\n",
    "        x_ot = None\n",
    "        for r in range(repeat):\n",
    "            data = x_ot_num.copy()\n",
    "            for c in range(8):\n",
    "                for m, v in transport_plan[c].items():\n",
    "                    data[data[:, c]==m, c] = v[r]\n",
    "            if x_ot is None:\n",
    "                x_ot = data\n",
    "            else:\n",
    "                x_ot = np.r_[x_ot, data]\n",
    "\n",
    "        # Perf on all test data\n",
    "        pred = best_prediction(clf, np.tile(target, (repeat, 1)), x_ot, feature_list, len(feature_list)-1, repeat=repeat)\n",
    "        perf = performance(pred, target_label)\n",
    "        bootstrap_perf.append(perf)\n",
    "        print(\"Bootstrap:\", perf, len(feature_list))\n",
    "    save_model(bootstrap_perf, \"./model/cate_bootstrap_perf_{}_{}\".format(ratio, s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
